# Intelligence Questions Generator

This application generates "Intelligence Questions" - the questions that a smart person would ask about a potential venture, with a focus on identifying risk.

## Overview

The application follows these steps:
1. Processing inputs (Market Chapter, PDFs, and Context)
2. Generating "Point of Maximum Skepticism" questions using different LLMs
3. Consolidating and refining these into 5 critical questions
4. Assessing risk for each question
5. (Future) Developing de-risking strategies

## Setup

### Prerequisites
- Python 3.7 or higher
- OpenRouter API key

### Installation

1. Clone this repository
2. Install the required packages:
   ```
   pip install -r requirements.txt
   ```
3. Create a `.env` file in the project directory with your OpenRouter API key:
   ```
   OPENROUTER_API_KEY=your_api_key_here
   ```

### Document Setup

The application is currently configured to use specific hardcoded file paths:

- Market Chapter: `C:\Users\hweth\OneDrive\Desktop\Innovera\Logo Project\pedram_intelligence\marketchapter.txt`
- Pitch Deck PDF: `C:\Users\hweth\OneDrive\Desktop\Innovera\Logo Project\pedram_intelligence\pitch_deck.pdf`
- Market Report PDF: `C:\Users\hweth\OneDrive\Desktop\Innovera\Logo Project\pedram_intelligence\market_report.pdf`

To use different files, modify these paths in the `get_user_input()` function in `intelligence_question_generator.py`.

## Test Mode

The application includes a test mode that uses only 2 LLMs instead of the full set of 10 models. This allows for faster testing of the pipeline without waiting for all models to respond.

To enable or disable test mode, modify the `TEST_MODE` constant at the top of `intelligence_question_generator.py`:
```python
# Test mode - when True, only uses 2 LLMs instead of all 10
TEST_MODE = True  # Set to False to use all 10 LLMs
```

The test mode uses the following models:
1. meta-llama/llama-4-maverick
2. qwen/qwq-32b

## Focus on Market Analysis

This application is specifically designed to generate questions about the MARKET aspects of a venture. It deliberately excludes questions about:
- Team
- Finances
- Operations
- Go-to-market strategy
- Other non-market aspects

These other aspects would be addressed in separate, specialized modules in a full implementation.

## Usage

Run the main script:
```
python intelligence_question_generator.py
```

The application will:
1. Load the Market Chapter from the specified text file
2. Prompt you to enter the Context information
3. Extract text from the specified PDF files
4. Save the extracted data to `extracted_data.json`
5. Ask if you want to proceed to Phase 2 (generating questions)
6. If yes, call the selected LLMs through OpenRouter to generate questions
7. Save the generated questions to `pms_questions.json`
8. Ask if you want to proceed to Phase 3 (consolidating questions)
9. If yes, use a high-reasoning model to consolidate the questions into 5 critical market-focused questions
10. Save the final questions to `final_questions.json`

## LLM Models Used

When running in regular mode (not test mode), the application uses the following 10 LLM models from OpenRouter:

1. openai/o1-mini
2. anthropic/claude-3.7-sonnet:thinking
3. google/gemini-2.5-flash-preview:thinking
4. x-ai/grok-3-beta
5. deepseek/deepseek-chat-v3-0324
6. arcee-ai/maestro-reasoning
7. qwen/qwq-32b
8. perplexity/sonar-reasoning-pro
9. meta-llama/llama-4-maverick
10. mistralai/mistral-medium-3

For question consolidation (Phase 3), a high-reasoning model is used: `anthropic/claude-3.7-sonnet:thinking`.

## Current Status

This is an MVP (Minimum Viable Product) version. Current functionality:
- Phase 1: Input processing and PDF text extraction ✓
- Phase 2: Generating PMS questions using LLMs via OpenRouter ✓
- Phase 3: Consolidating and refining questions with a high-reasoning model ✓
- Phase 4: Applying the risk assessment framework (Not implemented yet)
- Phase 5: Developing de-risking strategies (Not implemented yet)

## Output Files

The application generates several JSON files during execution:

- `extracted_data.json` - Contains the extracted text from the input files and the context
- `pms_questions.json` - Contains the questions generated by the LLMs (10 questions per model)
- `final_questions.json` - Contains the 5 final consolidated questions with reasoning 
